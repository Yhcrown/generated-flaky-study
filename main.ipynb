{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import datetime\n",
    "import time\n",
    "import logging\n",
    "import re\n",
    "import csv\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s', level=logging.DEBUG)\n",
    "\n",
    "# PROJECTS_DIRECTORY = \"/Users/yhcrown/Documents/flaky_java_projects/\"\n",
    "PROJECTS_DIRECTORY = \"/shared-data/generated-flaky/projects/\"\n",
    "CURRENT_DIRECTORY = os.getcwd()\n",
    "# RANDOOP_GENERATED_DIRECTORY = \"/Users/yhcrown/Documents/GitHub/generated-flaky-study/randoop_tests/\"\n",
    "RANDOOP_GENERATED_DIRECTORY = \"/shared-data/generated-flaky/randoop_tests/\"\n",
    "\n",
    "\n",
    "\n",
    "WORKSPACE='/workspace/generated-flaky-study/'\n",
    "# WORKSPACE= os.getcwd()\n",
    "# WORKSPACE=\"/shared-data/generated-flaky-study/\"\n",
    "\n",
    "# TOOLS_DIRECTORY = \"/Users/yhcrown/Documents/tools/\"\n",
    "TOOLS_DIRECTORY = \"/shared-data/common-jar/\"\n",
    "\n",
    "RANDOOP_JAR = TOOLS_DIRECTORY + 'randoop-all-4.3.3.jar'\n",
    "GUAVA_JAR = TOOLS_DIRECTORY + 'guava-33.0.0-jre.jar'\n",
    "HAMCREST_JAR = TOOLS_DIRECTORY + 'hamcrest-core-2.2.jar'\n",
    "JUNIT_JAR = TOOLS_DIRECTORY + 'junit-4.13.2.jar'\n",
    "SUMMARY_LOG = WORKSPACE + '/logs/'\n",
    "# SUMMARY_LOG = '/shared-data/generated-flaky/logs/'\n",
    "\n",
    "# MVN_LOC = \"/Users/yhcrown/Documents/tools/apache-maven-3.8.8/bin/mvn\"\n",
    "MVN_LOC = \"/workspace/apache-maven-3.8.8/bin/mvn\"\n",
    "\n",
    "# JAVA_HOME = \"/Library/Java/JavaVirtualMachines/zulu-8.jdk/Contents/Home\"\n",
    "JAVA_HOME = \"/shared-data/jdk8u422-b05/\"\n",
    "\n",
    "# INSTRUMENTED_JAVA_HOME = \"/Users/yhcrown/Library/Java/JavaVirtualMachines/java8-inst/\"\n",
    "INSTRUMENTED_JAVA_HOME = \"/shared-data/jdk-inst/\"\n",
    "\n",
    "FLAKYTRACKER_JAR = \"/shared-data/phosphor-flakyTracker/Phosphor/target/Phosphor-0.0.5-SNAPSHOT.jar\"\n",
    "# CONTROL_TRACK_JAVA_HOME = \"/Users/yhcrown/Library/Java/JavaVirtualMachines/java8-inst-controltrack\"\n",
    "CONTROL_TRACK_JAVA_HOME = \"/shared-data/jdk-inst-controltrack/\"\n",
    "\n",
    "\n",
    "generated_dataset_labeled_flaky_test = set()\n",
    "\n",
    "RERUN_TIMES = 10\n",
    "\n",
    "\n",
    "project_to_sha = dict()\n",
    "build_fails = set()\n",
    "generate_fails = set()\n",
    "flaky_projects = set()\n",
    "evosuite_flaky_projects = set()\n",
    "flaky_tests_per_project = dict()\n",
    "modified_flaky_tests_per_project = dict()\n",
    "line_content = dict()\n",
    "normal_test_num_per_project = dict()\n",
    "flaky_sta = dict()\n",
    "\n",
    "initial_flaky_test = dict()\n",
    "\n",
    "randoop_flaky_test = set()\n",
    "tracker_flaky_test = set()\n",
    "tracker_randoop_flaky_test = set()\n",
    "tracker_flaky_tests_except_randoop =  set()\n",
    "\n",
    "\n",
    "error_case_num = 0\n",
    "\n",
    "total_test_num = 0\n",
    "total_flaky_num = 0\n",
    "\n",
    "def read_dataset():\n",
    "    path = \"./dataset.csv\"\n",
    "    path = WORKSPACE+\"/dataset.csv\"\n",
    "    df = pd.read_csv(path)\n",
    "    developer_NOD_projects = df['Project_Name'].loc[df['flaky'] == 'NOD'].loc[df['language'] == 'Java'].loc[\n",
    "        df['test_type'] == 'developer-written'].unique()\n",
    "    cols_to_keep = ['Project_Name', 'Project_URL', 'Project_Hash']\n",
    "    developer_NOD_info = pd.DataFrame(\n",
    "        df[df['Project_Name'].isin(developer_NOD_projects)][cols_to_keep].drop_duplicates())\n",
    "    developer_NOD_info.to_csv(WORKSPACE+'/developer_NOD_projects_info.csv')\n",
    "    # print(developer_NOD_info)\n",
    "    test_info = df.loc[df['flaky'] == 'NOD'].loc[\n",
    "        df['language'] == 'Java'].loc[df['test_type'] == 'developer-written']\n",
    "    test_info =  test_info[test_info['Project_Name'].isin(developer_NOD_projects)]\n",
    "    # print(len(test_info))\n",
    "    # print(df['test_type'].unique())  ## ['evosuite_Default' 'evosuite_NoFlakinessSuppression' 'developer-written' 'generated']\n",
    "    # print(df['flaky'].unique())  ## not flaky, NOD,  OD\n",
    "    # print(developer_NOD_projects, len(developer_NOD_projects))  ## 105, same as paper\n",
    "    evo = df['Project_Name'].loc[df['flaky'] == 'NOD'].loc[df['language'] == 'Java'].loc[\n",
    "        df['test_type'] == 'evosuite_Default'].unique()\n",
    "    for p in evo:\n",
    "        if p in developer_NOD_projects:\n",
    "            evosuite_flaky_projects.add(p)\n",
    "    # print(\"evosuite:\", len(df['Project_Name'].loc[df['flaky'] == 'NOD'].loc[df['language'] == 'Java'].loc[\n",
    "    #                            df['test_type'] == 'evosuite_NoFlakinessSuppression'].unique()))\n",
    "    \n",
    "    filtered_df = df[(df['language'] == 'Java') & (df['test_type'] == 'developer-written') & (df['flaky'] == 'NOD')]\n",
    "    print(filtered_df)\n",
    "\n",
    "    return developer_NOD_info, filtered_df\n",
    "\n",
    "\n",
    "def download_project(project, target_dir):\n",
    "    cwd = os.getcwd()\n",
    "    url = project['Project_URL']\n",
    "    name = project['Project_Name']\n",
    "    commit = project['Project_Hash']\n",
    "    # if os.path.isdir(target_dir):\n",
    "    #     shutil.rmtree(target_dir)\n",
    "    os.chdir(PROJECTS_DIRECTORY)\n",
    "    subprocess.run('git clone ' +url.strip() + ' ' + name, shell=True, stdout=open(os.devnull, 'w'), stderr=subprocess.STDOUT)\n",
    "    os.chdir(target_dir)\n",
    "    subprocess.run('git checkout ' + commit, shell=True, stdout=open(os.devnull, 'w'), stderr=subprocess.STDOUT)\n",
    "    os.chdir(cwd)\n",
    "\n",
    "\n",
    "def build_project(target_dir):\n",
    "    cwd = os.getcwd()\n",
    "    os.chdir(target_dir)\n",
    "    build_log = target_dir + '/build.log'\n",
    "    start_time = time.time()\n",
    "    print('Building client ... ' + str(datetime.datetime.now()))\n",
    "    os.environ['JAVA_HOME'] = JAVA_HOME\n",
    "    subprocess.run(\n",
    "        MVN_LOC + ' install -DskipTests -Ddependency-check.skip=true -Denforcer.skip=true -Drat.skip=true -Dmdep.analyze.skip=true -Dmaven.javadoc.skip=true -Dgpg.skip=true -Dlicense.skip=true -am  ',\n",
    "        shell=True, stdout=open(build_log, 'w'), stderr=subprocess.STDOUT)\n",
    "    end_time = time.time()\n",
    "    insertTimeInLog(start_time, end_time, build_log)\n",
    "    os.chdir(cwd)\n",
    "\n",
    "\n",
    "def run_randoop(project, target_dir):\n",
    "    cwd = os.getcwd()\n",
    "    os.chdir(target_dir)\n",
    "    if os.path.isdir('/tmp/jars'):\n",
    "        shutil.rmtree('/tmp/jars')\n",
    "    os.mkdir('/tmp/jars')\n",
    "\n",
    "    start_time = time.time()\n",
    "    os.environ['JAVA_HOME'] = JAVA_HOME\n",
    "    subprocess.run(MVN_LOC+ ' dependency:copy-dependencies',\n",
    "                   shell=True, stdout=open(os.devnull, 'w'), stderr=subprocess.STDOUT)\n",
    "    for dir, subdir, files in os.walk(target_dir):\n",
    "        if \"dependency\" in subdir:\n",
    "            for file in os.listdir(dir + \"/dependency\"):\n",
    "                if file.endswith(\".jar\"):\n",
    "                    shutil.copy(dir + \"/dependency/\" + file, \"/tmp/jars\")\n",
    "    os.chdir(target_dir)\n",
    "\n",
    "    ## Linux platform\n",
    "    concat_class_path = '$(find ' + target_dir + ' -name \\\"classes\\\" -type d | paste -sd :)'\n",
    "    concat_class_path += ':$(find ' + target_dir + ' -name \\\"test-classes\\\" -type d | paste -sd :)'\n",
    "    concat_class_path += ':$(find /tmp/jars -name \\\"*.jar\\\" -type f | paste -sd :):'\n",
    "    # print(concat_class_path)\n",
    "    ## Mac os platform\n",
    "\n",
    "    # concat_class_path = '$(find ' + target_dir + ' -name \"classes\" -type d | xargs echo | tr \\' \\' \\':\\')'\n",
    "    # concat_class_path += ':$(find ' + target_dir + ' -name \"test-classes\" -type d | xargs echo | tr \\' \\' \\':\\')'\n",
    "    # concat_class_path += ':$(find /tmp/jars -name \"*.jar\" -type f | xargs echo | tr \\' \\' \\':\\'):'\n",
    "\n",
    "    generated_dir = RANDOOP_GENERATED_DIRECTORY + project['Project_Name'] + '/' + project['Project_Hash'] + '/alltests/'\n",
    "    if not os.path.exists(generated_dir):\n",
    "        os.makedirs(generated_dir)\n",
    "    literals_file = generated_dir + 'literal.log'\n",
    "    test_method_num_limit = 500\n",
    "    test_method_max_size = 100\n",
    "    class_list_file = '/tmp/classes.txt'\n",
    "    all_classes = []\n",
    "    # print(target_dir)\n",
    "    for dir_path, subpaths, files in os.walk(target_dir):\n",
    "        for f in files:\n",
    "\n",
    "            if f.endswith('.class') and '/classes/' in dir_path:\n",
    "                clz = (dir_path + '/' + f.split('.')[0]).split('/classes/')[-1].replace('/', '.')\n",
    "                if clz not in all_classes:\n",
    "                    all_classes.append(clz)\n",
    "    # print(all_classes)\n",
    "\n",
    "    with open(class_list_file, 'w') as fw:\n",
    "        for clz in all_classes:\n",
    "            if '$' in clz:\n",
    "                # print(clz)\n",
    "                clz = clz.split('$')[0]\n",
    "            fw.write(clz + '\\n')\n",
    "\n",
    "    concat_class_path += RANDOOP_JAR + ':'\n",
    "    concat_class_path += JUNIT_JAR + ':'\n",
    "    concat_class_path += HAMCREST_JAR + ':'\n",
    "    concat_class_path += GUAVA_JAR\n",
    "\n",
    "    os.environ['JAVA_HOME'] = JAVA_HOME\n",
    "    os.environ['PATH'] = f\"{JAVA_HOME}/bin:{os.environ['PATH']}\"\n",
    "    randoop_cmd = JAVA_HOME+'/bin/java -ea -classpath ' + concat_class_path + \\\n",
    "                  ' randoop.main.Main gentests' \\\n",
    "                  + ' --classlist=' + class_list_file \\\n",
    "                  + ' --output-limit=' + str(test_method_num_limit) \\\n",
    "                  + ' --time-limit=60' \\\n",
    "                  + ' --junit-output-dir=' + generated_dir \\\n",
    "                  + ' --regression-test-basename=TestGroup' \\\n",
    "                  + str(test_method_max_size) + 'Case' \\\n",
    "                  + ' --jvm-max-memory=1024m ' \\\n",
    "                  + ' --log=' + str(generated_dir) + 'randoop-log.txt' \\\n",
    "                  + ' --usethreads=true' \\\n",
    "        # + ' --literals-file=' + literals_file \\\n",
    "    # + ' --literals-level=ALL'\n",
    "    logging.info(randoop_cmd)\n",
    "    test_gen_log = generated_dir + '/testgen.txt'\n",
    "    try:\n",
    "        subprocess.run(randoop_cmd, shell=True, stdout=open(test_gen_log, 'w'), stderr=subprocess.STDOUT, timeout=90)\n",
    "    except subprocess.TimeoutExpired as e:\n",
    "        with open(test_gen_log, \"a+\") as f:\n",
    "            f.write(str(e))\n",
    "        print(project['Project_Name'], e)\n",
    "    end_time = time.time()\n",
    "    insertTimeInLog(start_time, end_time, test_gen_log)\n",
    "    os.chdir(cwd)\n",
    "\n",
    "def run_flaky_tracker(project, target_dir, module = None):\n",
    "    cwd = os.getcwd()\n",
    "    os.chdir(target_dir)\n",
    "    if module:\n",
    "        os.chdir(target_dir + '/' + module)\n",
    "    if os.path.isdir('/tmp/jars'):\n",
    "        shutil.rmtree('/tmp/jars')\n",
    "    os.mkdir('/tmp/jars')\n",
    "\n",
    "    start_time = time.time()\n",
    "    os.environ['JAVA_HOME'] = JAVA_HOME\n",
    "    subprocess.run(MVN_LOC+ ' dependency:copy-dependencies',\n",
    "                   shell=True, stdout=open(os.devnull, 'w'), stderr=subprocess.STDOUT)\n",
    "    for dir, subdir, files in os.walk(target_dir):\n",
    "        if \"dependency\" in subdir:\n",
    "            for file in os.listdir(dir + \"/dependency\"):\n",
    "                if file.endswith(\".jar\"):\n",
    "                    shutil.copy(dir + \"/dependency/\" + file, \"/tmp/jars\")\n",
    "    os.chdir(target_dir)\n",
    "\n",
    "    ## Linux platform\n",
    "    concat_class_path = '$(find ' + target_dir + ' -name \\\"classes\\\" -type d | paste -sd :)'\n",
    "    concat_class_path += ':$(find ' + target_dir + ' -name \\\"test-classes\\\" -type d | paste -sd :)'\n",
    "    concat_class_path += ':$(find /tmp/jars -name \\\"*.jar\\\" -type f | paste -sd :):'\n",
    "    # print(concat_class_path)\n",
    "    ## Mac os platform\n",
    "\n",
    "    # concat_class_path = '$(find ' + target_dir + ' -name \"classes\" -type d | xargs echo | tr \\' \\' \\':\\')'\n",
    "    # concat_class_path += ':$(find ' + target_dir + ' -name \"test-classes\" -type d | xargs echo | tr \\' \\' \\':\\')'\n",
    "    # concat_class_path += ':$(find /tmp/jars -name \"*.jar\" -type f | xargs echo | tr \\' \\' \\':\\'):'\n",
    "\n",
    "\n",
    "\n",
    "    generated_dir = RANDOOP_GENERATED_DIRECTORY + project['Project_Name'] + '/' + project['Project_Hash'] + '/alltests/'\n",
    "\n",
    "\n",
    "\n",
    "    if not os.path.exists(generated_dir):\n",
    "        os.makedirs(generated_dir)\n",
    "    literals_file = generated_dir + 'literal.log'\n",
    "    class_list_file = '/tmp/classes.txt'\n",
    "    all_classes = []\n",
    "\n",
    "    for dir_path, subpaths, files in os.walk(target_dir):\n",
    "        for f in files:\n",
    "\n",
    "            if f.endswith('.class') and ('/classes/' in dir_path or '/test-classes/' in dir_path):\n",
    "                clz = (dir_path + '/' + f.split('.')[0]).split('/classes/')[-1].replace('/', '.')\n",
    "                if clz not in all_classes:\n",
    "                    all_classes.append(clz)\n",
    "    # print(all_classes)\n",
    "\n",
    "    with open(class_list_file, 'w') as fw:\n",
    "        for clz in all_classes:\n",
    "            if '$' in clz:\n",
    "                # print(clz)\n",
    "                clz = clz.split('$')[0]\n",
    "            fw.write(clz + '\\n')\n",
    "\n",
    "    concat_class_path += RANDOOP_JAR + ':'\n",
    "    concat_class_path += JUNIT_JAR + ':'\n",
    "    concat_class_path += HAMCREST_JAR + ':'\n",
    "    concat_class_path += GUAVA_JAR\n",
    "\n",
    "    # os.environ['JAVA_HOME'] = JAVA_HOME\n",
    "    test_dir = target_dir + '/src/test/java/flaky'\n",
    "\n",
    "    for dir, subpath, files in os.walk(RANDOOP_GENERATED_DIRECTORY+project['Project_Name']):\n",
    "        for file in files:\n",
    "            if file == 'TestGroup100Case0.java':\n",
    "                # os.remove(dir+'/'+'flaky_tests.java')\n",
    "                with open(dir + '/' + file, 'r+') as f:\n",
    "                    original_lines = f.readlines()\n",
    "                    new_lines = []\n",
    "                    new_lines.append(\"package flaky; \\n\")\n",
    "                    for line in original_lines:\n",
    "                        if \"import org.junit.FixMethodOrder;\" in line  or \"import org.junit.runners.MethodSorters;\" in line or \"@FixMethodOrder(MethodSorters.NAME_ASCENDING)\" in line:\n",
    "                            continue\n",
    "                        if line.startswith(\"public class\"):\n",
    "                            new_lines.append(\"public class RandoopTest{\")\n",
    "                        else:\n",
    "                            # maybe more process\n",
    "                            new_lines.append(line)\n",
    "                    new_class = \"\\n\".join(new_lines)\n",
    "                with open(dir + '/RandoopTest.java', 'w+') as f:\n",
    "                    f.write(new_class)\n",
    "                if not os.path.exists(test_dir):\n",
    "                    os.mkdir(test_dir)\n",
    "                shutil.copy(dir+ '/RandoopTest.java', target_dir+'/src/test/java/flaky/RandoopTest.java')\n",
    "            if file == 'FlakyTest.java':\n",
    "                if not os.path.exists(test_dir):\n",
    "                    os.mkdir(test_dir)\n",
    "                shutil.copy(dir+ '/FlakyTest.java', target_dir+'/src/test/java/flaky/FlakyTest.java')\n",
    "\n",
    "    build_log = target_dir + '/buildFlaky.log'\n",
    "    # start_time = time.time()\n",
    "    os.environ['JAVA_HOME'] = JAVA_HOME\n",
    "    # os.environ['MAVEN_OPT'] = \"-Xbootclasspath/a:\" + FLAKYTRACKER_JAR + \" -javaagent:\"+FLAKYTRACKER_JAR\n",
    "    os.chdir(target_dir)\n",
    "    # subprocess.run(MVN_LOC+ ' -version', executable='/bin/zsh', shell=True, stdout=subprocess.STDOUT, stderr=subprocess.STDOUT)\n",
    "    # subprocess.run(JAVA_HOME+'/bin/javac -cp '+concat_class_path+' '+target_dir+'/src/test/java/flaky/FlakyTest.java', executable='/bin/zsh', shell=True,\n",
    "    #                stderr=subprocess.STDOUT, timeout=90, stdout=open(build_log, 'w'))\n",
    "    # print(JAVA_HOME+'/bin/javac -cp '+concat_class_path+' '+target_dir+'/src/test/java/flaky/FlakyTest.java')\n",
    "    subprocess.run(\n",
    "        MVN_LOC + ' install -DskipTests -Ddependency-check.skip=true -Denforcer.skip=true -Drat.skip=true -Dmdep.analyze.skip=true -Dmaven.javadoc.skip=true -Dgpg.skip=true -Dlicense.skip=true ',\n",
    "        shell=True, stdout=open(build_log, 'w'), stderr=subprocess.STDOUT)\n",
    "    if os.path.exists(generated_dir+'/flakyTracker/'):\n",
    "        shutil.rmtree(generated_dir+'/flakyTracker/')\n",
    "    #find all test-classes\n",
    "    for dir, subpath, files in os.walk(target_dir+'/target/test-classes'):\n",
    "        for file in files:\n",
    "            if file.endswith(\".class\"):   #change here to '.class' to make it run all classes\n",
    "                class_name = os.path.join(dir, file)\n",
    "                class_name = class_name.replace(target_dir+'/target/test-classes/','').replace('/', '.').replace('\\\\', '.').replace('.class', '')\n",
    "                # print(class_name)\n",
    "                flaky_tracker_cmd = CONTROL_TRACK_JAVA_HOME + \"/bin/java -javaagent:\" + FLAKYTRACKER_JAR + \" -Xbootclasspath/a:\" + FLAKYTRACKER_JAR + \"  -cp \" + concat_class_path + \" org.junit.runner.JUnitCore \"+class_name\n",
    "\n",
    "                # flaky_tracker_dir =\n",
    "\n",
    "                flaky_tracker_log = generated_dir + '/flakyTracker/' + class_name.replace('.','/')+'.trackerlog'\n",
    "                flaky_tracker_dir = '/'.join(flaky_tracker_log.split('/')[0:len(flaky_tracker_log.split('/'))-1])\n",
    "                # os.path.dirname(flaky_tracker_log)\n",
    "                print(flaky_tracker_log)\n",
    "                if not os.path.exists(flaky_tracker_dir):\n",
    "                    os.makedirs(flaky_tracker_dir)\n",
    "                try:\n",
    "                    subprocess.run(flaky_tracker_cmd, shell=True, stdout=open(flaky_tracker_log, 'w'),\n",
    "                                   stderr=subprocess.STDOUT, timeout=600)\n",
    "                except subprocess.TimeoutExpired as e:\n",
    "                    with open(flaky_tracker_log, \"a+\") as f:\n",
    "                        f.write(str(e))\n",
    "                    print(project['Project_Name'], e)\n",
    "                # print(flaky_tracker_cmd)\n",
    "\n",
    "                # break\n",
    "\n",
    "tracker_data = []\n",
    "random_num = 0\n",
    "time_num = 0\n",
    "\n",
    "tracker_data_set = set()\n",
    "\n",
    "# 创建新元素的哈希标识\n",
    "\n",
    "\n",
    "def parseTrackerLog(log_path):\n",
    "    for dir, subpath, files in os.walk(log_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".trackerlog\"):\n",
    "                # index = dir.find(RANDOOP_GENERATED_DIRECTORY)\n",
    "                subdir = dir[len(RANDOOP_GENERATED_DIRECTORY):]\n",
    "                info = subdir.split('/')\n",
    "                project_name = info[0]\n",
    "                hash = info[1]\n",
    "                class_name = os.path.join(dir, file)\n",
    "                class_name = class_name.split(\"/flakyTracker/\")[1].replace('/', '.').replace('\\\\', '.').replace('.trackerlog', '')\n",
    "                # class_name = class_name.replace(target_dir+'/target/test-classes/','')\n",
    "   \n",
    "                generated_dir = RANDOOP_GENERATED_DIRECTORY + project['Project_Name'] + '/' + project['Project_Hash'] + '/alltests/'\n",
    "   \n",
    "                if os.path.exists(dir+file):\n",
    "                    shutil\n",
    "                with open(dir+'/'+file,\"r+\") as f:\n",
    "                    lines = f.readlines()\n",
    "                    pattern = re.compile(\n",
    "                        r'FlakyTracker Log: (\\S+)\\s+may be flaky:\\s+FlakyTaintLabel{type=(\\S+),\\s+cause=(\\S+),\\s+file=(\\S+),\\s+line=(-?\\d+),\\s+label=(\\d+)}')\n",
    "                    matches = pattern.findall('\\n'.join(lines))\n",
    "                    randoop_test_pattern = r'^test\\d+(_\\d+)$'\n",
    "                    for match in matches:\n",
    "                        test_name, flaky_type, cause, file, line, label = match\n",
    "                        test_name = re.sub(r'^[\\.E]+', '', test_name)  ## old version\n",
    "                        if 'flaky.FlakyTest' in class_name or 'flaky.RandoopTest' in class_name: ## is randoop test\n",
    "                            tracker_randoop_flaky_test.add(project_name+'#'+test_name)\n",
    "                        else:\n",
    "                            tracker_flaky_tests_except_randoop.add(project_name+'#'+class_name+'#'+test_name)\n",
    "                            \n",
    "                            \n",
    "                        global random_num\n",
    "                        global time_num\n",
    "                        if project_name+'#'+test_name not in tracker_flaky_test:\n",
    "                            if 'RANDOM' in flaky_type:\n",
    "                                random_num = random_num + 1\n",
    "                            if 'TIME' in flaky_type:\n",
    "                                time_num = time_num + 1\n",
    "                            tracker_flaky_test.add(project_name+'#'+test_name)\n",
    "\n",
    "                        new_entry_hash = (project_name, hash, test_name, flaky_type, cause, file, int(line), int(label))\n",
    "\n",
    "                        if  ('flaky.FlakyTest' in class_name or 'flaky.RandoopTest' in class_name): ## is randoop test\n",
    "                            continue\n",
    "                        if new_entry_hash not in tracker_data_set:\n",
    "                            tracker_data.append({\n",
    "                                'Project Name': project_name,\n",
    "                                'sha': hash,\n",
    "                                'Test Name': test_name,\n",
    "                                'Type': flaky_type,\n",
    "                                'Cause': cause,\n",
    "                                'File': file,\n",
    "                                'Line': int(line),\n",
    "                                'Label': int(label)\n",
    "                            })\n",
    "                            # 同时将哈希标识添加到集合中 \n",
    "                            tracker_data_set.add(new_entry_hash)\n",
    "\n",
    "    df = pd.DataFrame(tracker_data)\n",
    "    print(df)\n",
    "    df.to_csv(SUMMARY_LOG+'tracker_analysis.csv')\n",
    "    # with open(SUMMARY_LOG + 'tracker_analysis.csv', 'w+', newline='') as wf:\n",
    "    #     writer = csv.writer(wf)\n",
    "    #     writer.writerow(['project','commit','Test','type','cause','file','line','label'])\n",
    "    #     for entry in df:\n",
    "    #         fp = df[entry]\n",
    "    #         writer.writerow([project.replace('-','/',1),str(project_to_sha[project])[0:7],normal_test_num_per_project[project],len(fp)])\n",
    "\n",
    "\n",
    "# def make_tracker_table():\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def insertTimeInLog(start_time, end_time, log):\n",
    "    duration = (end_time - start_time)\n",
    "    fr = open(log, 'r')\n",
    "    lines = fr.readlines()\n",
    "    fr.close()\n",
    "    lines.insert(0, '[TIME]: ' + str(datetime.timedelta(seconds=duration)) + '\\n')\n",
    "    fw = open(log, 'w')\n",
    "    fw.write(''.join(lines))\n",
    "    fw.close()\n",
    "\n",
    "\n",
    "def search_build_error():\n",
    "    log = open(SUMMARY_LOG + 'build_summary.log', 'w+')\n",
    "    global error_case_num\n",
    "    for dir, subpath, files in os.walk(PROJECTS_DIRECTORY):\n",
    "        for file in files:\n",
    "            if file == 'build.log':\n",
    "                with open(dir + '/' + file, 'r') as f:\n",
    "                    lines = f.readlines()\n",
    "                    for line in lines:\n",
    "                        if '[INFO] BUILD FAILURE' in line:\n",
    "                            log.write(dir.split(\"/\")[-1] + ', BUILD FAILURE\\n')\n",
    "                            build_fails.add(dir.split('/')[-1])\n",
    "                            # error_case_num = error_case_num +1\n",
    "                        if '[ERROR]' in line:\n",
    "                            log.write(line)\n",
    "    log.close()\n",
    "\n",
    "\n",
    "def search_generate_error():\n",
    "    log = open(SUMMARY_LOG + 'generated_summary.log', 'w+')\n",
    "    for dir, subpath, files in os.walk(PROJECTS_DIRECTORY):\n",
    "        for file in files:\n",
    "            if file == 'build.log':\n",
    "                with open(dir + '/' + file, 'r') as f:\n",
    "                    lines = f.readlines()\n",
    "                    for line in lines:\n",
    "                        if '[INFO] BUILD FAILURE' in line:\n",
    "                            log.write(dir.split(\"/\")[-1] + ', BUILD FAILURE\\n')\n",
    "                            build_fails.add(dir.split('/')[-1])\n",
    "                        if '[ERROR]' in line:\n",
    "                            log.write(line)\n",
    "    log.close()\n",
    "\n",
    "\n",
    "def search_error_cause():\n",
    "    global error_case_num\n",
    "    for dir, subpath, files in os.walk(RANDOOP_GENERATED_DIRECTORY):\n",
    "        for file in files:\n",
    "            if file == \"testgen.txt\":\n",
    "                if len(files) < 4:\n",
    "                    print(\"no generated test in project:\", dir.split(\"/\")[-3])\n",
    "                    error_case_num = error_case_num + 1\n",
    "                    generate_fails.add(dir.split('/')[-3])\n",
    "                    if dir.split(\"/\")[-3] not in build_fails:\n",
    "                        print(\"-----\", dir.split(\"/\")[-3])\n",
    "                    break\n",
    "\n",
    "\n",
    "def find_flaky():\n",
    "    log = open(SUMMARY_LOG + 'flaky_summary.log', 'w+')\n",
    "    log2 = open(SUMMARY_LOG + 'flaky_line.log', 'w+')\n",
    "    for dir, subpath, files in os.walk(RANDOOP_GENERATED_DIRECTORY):\n",
    "        for file in files:\n",
    "            if file == \"testgen.txt\":\n",
    "                with open(dir + '/' + file, 'r') as f:\n",
    "                    lines = f.readlines()\n",
    "                    first = True\n",
    "                    for line in lines:\n",
    "                        if 'Possibly flaky:' in line:\n",
    "                            if first:\n",
    "                                log.write(dir.split(\"/\")[-3] + '\\n')\n",
    "                                first = False\n",
    "                                flaky_projects.add(dir.split('/')[-3])\n",
    "                            log.write(line)\n",
    "    pattern = r'@Test.*?// flaky\\s*\"\\d+\\)\\s+\\w+\\(\\w+\\)\"\\s*:.*?(?=@Test|\\Z)'\n",
    "\n",
    "    global total_flaky_num\n",
    "    global total_test_num\n",
    "    for project in flaky_projects:\n",
    "        for dir, subpath, files in os.walk(RANDOOP_GENERATED_DIRECTORY + \"/\" + project):\n",
    "            for file in files:\n",
    "                if file == \"TestGroup100Case0.java\":\n",
    "                    with open(dir + '/' + file, 'r') as f:\n",
    "                        java_code = f.read()\n",
    "                        total_test_num += len(java_code.split(\"@Test\"))\n",
    "                        if project not in normal_test_num_per_project:\n",
    "                            normal_test_num_per_project[project] = len(java_code.split(\"@Test\"))\n",
    "                        matches = re.findall(pattern, java_code, re.DOTALL)\n",
    "                        project_name = dir.split(\"/\")[-3]\n",
    "                        # log2.write(project_name + \", length: \"+str(len(matches))+\"\\n\")\n",
    "\n",
    "                        for match in matches:\n",
    "                            test_case = match.strip().split('@Test')[-1]\n",
    "                            total_flaky_num += 1\n",
    "                            if project not in flaky_tests_per_project:\n",
    "                                flaky_tests_per_project[project] = list()\n",
    "                            flaky_tests_per_project[project].append(test_case)\n",
    "                            # print(test_case)\n",
    "                            # log2.write(test_case+\"\\n\")\n",
    "                        if 'DiUS' in project:\n",
    "                            print()\n",
    "                        last_test = java_code.strip().split('@Test')[-1]\n",
    "                        for line in last_test.split('\\n'):\n",
    "                            if re.search(r'// flaky\\s*\"\\d+\\)\\s+\\w+\\(\\w+\\)\"\\s*:', line):\n",
    "                                flaky_tests_per_project[project][-1] = flaky_tests_per_project[project][-1][:-2]\n",
    "                                break   ##remove the duplicate }\n",
    "                                # if project not in flaky_tests_per_project:\n",
    "                                #     flaky_tests_per_project[project] = list()\n",
    "                                # flaky_tests_per_project[project].append(last_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def uncomment_tests():\n",
    "    for flaky_project in flaky_projects:\n",
    "        tests = flaky_tests_per_project[flaky_project]\n",
    "        modified_flaky_tests_per_project[flaky_project] = list()\n",
    "\n",
    "        for test_code in tests:\n",
    "            versions = []\n",
    "            lines = test_code.split('\\n')\n",
    "            flaky_lines = [i for i, line in enumerate(lines) if re.search(r'// flaky\\s*\"\\d+\\)\\s+\\w+\\(\\w+\\)\"\\s*:', line)]\n",
    "            for i, line_num in enumerate(flaky_lines):\n",
    "                version = lines.copy()\n",
    "                version[line_num] = re.sub(r'// flaky\\s*\"\\d+\\)\\s+\\w+\\(\\w+\\)\"\\s*:\\s*', '', version[line_num])\n",
    "                new_code = '\\n'.join(version)\n",
    "                test_name = re.search(r'public void\\s+(\\w+)\\s*\\(', test_code).group(1)\n",
    "                modified_test_name = f\"{test_name}_{i + 1}\"\n",
    "\n",
    "                randoop_flaky_test.add(flaky_project+'#'+modified_test_name)\n",
    "\n",
    "                modified_test_code = re.sub(r'public void\\s+' + test_name + r'\\s*\\(',\n",
    "                                            'public void ' + modified_test_name + '(',\n",
    "                                            new_code)\n",
    "                modified_flaky_tests_per_project[flaky_project].append(modified_test_code)\n",
    "                line_content[flaky_project+'_'+modified_test_name.split('.')[-1]] = version[line_num]\n",
    "\n",
    "def generate_new_testclass():\n",
    "    for project in flaky_projects:\n",
    "        for dir, subpath, files in os.walk(RANDOOP_GENERATED_DIRECTORY + \"/\" + project):\n",
    "            for file in files:\n",
    "                if file == 'TestGroup100Case0.java':\n",
    "                    # os.remove(dir+'/'+'flaky_tests.java')\n",
    "                    with open(dir + '/' + file, 'r+') as f:\n",
    "                        original_lines = f.readlines()\n",
    "                        new_lines = []\n",
    "                        new_lines.append(\"package flaky; \\n\")\n",
    "                        for line in original_lines:\n",
    "                            if \"import org.junit.FixMethodOrder;\" in line  or \"import org.junit.runners.MethodSorters;\" in line or \"@FixMethodOrder(MethodSorters.NAME_ASCENDING)\" in line:\n",
    "                                continue\n",
    "                            if line.startswith(\"import\") or \"public static boolean debug = false;\" in line:\n",
    "                                new_lines.append(line)\n",
    "                            elif line.startswith(\"public class\"):\n",
    "                                new_lines.append(\"public class FlakyTest{\")\n",
    "                            elif \"@Test\" in line:\n",
    "                                break\n",
    "                        new_class = '\\n'.join(new_lines)\n",
    "                        for test in modified_flaky_tests_per_project[project]:\n",
    "                            new_class += \"\\n\" + \"\\t@Test\" + test\n",
    "                        new_class += '\\n}'\n",
    "                    with open(dir + '/FlakyTest.java', 'w+') as f:\n",
    "                        f.write(new_class)\n",
    "\n",
    "\n",
    "\n",
    "def copy_and_run():\n",
    "    cwd = os.getcwd()\n",
    "    for project in flaky_projects:\n",
    "        target_dir = PROJECTS_DIRECTORY + project\n",
    "        os.chdir(target_dir)\n",
    "        test_dir = target_dir + '/src/test/java/flaky'\n",
    "        if not os.path.exists(test_dir):\n",
    "            os.mkdir(test_dir)\n",
    "        for dir, subpath, files in os.walk(RANDOOP_GENERATED_DIRECTORY + \"/\" + project):\n",
    "            if 'FlakyTest.java' in files:\n",
    "                log_dir = dir + '/rerun/'\n",
    "                if not os.path.exists(log_dir):\n",
    "                    os.mkdir(log_dir)\n",
    "                if 'FlakyTests.java' in files:\n",
    "                    os.remove(dir+'/FlakyTests.java')\n",
    "                if os.path.exists(test_dir+'FlakyTests.java'):\n",
    "                    os.remove(test_dir+'/FlakyTests.java')\n",
    "                shutil.copy(dir + '/FlakyTest.java', test_dir + '/FlakyTest.java')\n",
    "\n",
    "        # cmd = MVN_LOC+ ' -Dtest=FlakyTest test '\n",
    "        # cmd2 = MVN_LOC+ ' -Dtest=FlakyTest surefire:test'\n",
    "        # print(os.getcwd(), cmd)\n",
    "        # if not os.path.exists(log_dir + '/all'):\n",
    "        #     os.mkdir(log_dir + '/all')\n",
    "        # subprocess.run(cmd, shell=True, stdout=open(log_dir + '/all/' + '0.log', 'w+'),\n",
    "        #                stderr=subprocess.STDOUT, timeout=90)\n",
    "        # for i in range(1, RERUN_TIMES):\n",
    "        #     subprocess.run(cmd2, shell=True, stdout=open(log_dir + '/all/' + str(i) + '.log', 'w+'),\n",
    "        #                    stderr=subprocess.STDOUT, timeout=90)\n",
    "\n",
    "\n",
    "def collect_flaky():\n",
    "    # for project in flaky_projects:\n",
    "    csv_log = SUMMARY_LOG + '/flaky_rerun.csv'\n",
    "    with open(csv_log, 'w+', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['project','sha','test','line','message'])\n",
    "        for dir, subpath, files in os.walk(RANDOOP_GENERATED_DIRECTORY):\n",
    "                if dir.endswith('/all'):\n",
    "                    for log_file in files:\n",
    "                        project_name = dir.split('/')[-5]\n",
    "                        project_sha = dir.split('/')[-4]\n",
    "                        project_to_sha[project_name] = project_sha\n",
    "                        with open(dir + '/' + log_file, 'r+') as f:\n",
    "                            log_content = f.read()\n",
    "                            # 提取test部分的信息\n",
    "\n",
    "                            test_results = re.findall(r'FlakyTest.(.*?)\\:(\\d+)\\s+(.*?)$', log_content, re.MULTILINE)\n",
    "                            for test_result in test_results:\n",
    "                                test_name = test_result[0]\n",
    "                                line_number = test_result[1]\n",
    "                                error_message = test_result[2]\n",
    "\n",
    "                                writer.writerow([project_name,project_sha,test_name, line_number, error_message])\n",
    "\n",
    "\n",
    "def statistic_flaky():\n",
    "    # flaky_sta = dict()\n",
    "    with open(SUMMARY_LOG+'flaky_rerun.csv','r+') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)\n",
    "        for row in reader:\n",
    "            name = row[0]\n",
    "            if name not in flaky_sta:\n",
    "                flaky_sta[name] = dict()\n",
    "                flaky_sta[name]['name'] = name\n",
    "                flaky_sta[name]['test_sta'] = dict()\n",
    "                flaky_sta[name]['sha'] = row[1]\n",
    "            flaky_project = flaky_sta[name]\n",
    "            test_name = row[2]\n",
    "            if test_name not in flaky_project['test_sta']:\n",
    "                flaky_project['test_sta'][test_name] = dict()\n",
    "                flaky_project['test_sta'][test_name]['line'] = row[3]\n",
    "                flaky_project['test_sta'][test_name]['wrong_times'] = 0\n",
    "            flaky_project['test_sta'][test_name]['wrong_times']+= 1\n",
    "    with open(SUMMARY_LOG + 'flaky_rerun_all.csv', 'w+',newline='') as wf:\n",
    "        writer = csv.writer(wf)\n",
    "        writer.writerow(['project', 'sha', 'test', 'line', 'run_times', 'wrong_times', 'is_flaky','line_content'])\n",
    "        for name in flaky_sta:\n",
    "            flaky_num = 0\n",
    "            sha = flaky_sta[name]['sha']\n",
    "            for test in flaky_sta[name]['test_sta']:\n",
    "                wrong_times = flaky_sta[name]['test_sta'][test]['wrong_times']\n",
    "                line = flaky_sta[name]['test_sta'][test]['line']\n",
    "                is_flaky = \"flaky\" if wrong_times != RERUN_TIMES else \"not flaky\"\n",
    "                if is_flaky == 'flaky':\n",
    "                    flaky_num +=1\n",
    "                writer.writerow([name,sha,test,line,RERUN_TIMES,wrong_times,is_flaky,line_content[name+'_'+test.split('.')[-1]]])\n",
    "            flaky_sta[name]['flaky_num'] = flaky_num\n",
    "\n",
    "    for project in flaky_projects:\n",
    "        print(project,\"original flaky test number: \", len(flaky_tests_per_project[project]) )\n",
    "        if project in flaky_sta:\n",
    "            print(\" now the flaky test number \",len(flaky_sta[project]['test_sta']),\"real flaky test number \",flaky_sta[project]['flaky_num'])\n",
    "        else:\n",
    "            print(\"no test was tested in project:\",project)\n",
    "\n",
    "def make_flaky_table():\n",
    "    with open(SUMMARY_LOG + 'flaky_per_project.csv', 'w+', newline='') as wf:\n",
    "        writer = csv.writer(wf)\n",
    "        writer.writerow(['project','commit','test_num','flaky_num'])\n",
    "        for project in flaky_tests_per_project:\n",
    "            fp = flaky_tests_per_project[project]\n",
    "            writer.writerow([project.replace('-','/',1),str(project_to_sha[project])[0:7],normal_test_num_per_project[project],len(fp)])\n",
    "\n",
    "\n",
    "verified_flaky_tests = defaultdict(lambda: defaultdict(int))\n",
    "verified_tests_set = set()\n",
    "# Function to run Maven test command\n",
    "def run_maven_test(project_dir, test_class, report_dir):\n",
    "    os.chdir(project_dir)\n",
    "    \n",
    "    command = f\"{MVN_LOC} -Dtest={test_class} -Dsurefire.reportNameSuffix={report_dir} test\"\n",
    "    command = f\"{MVN_LOC} -Dsurefire.reportNameSuffix={report_dir} test\"\n",
    "    result = subprocess.run(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    return result.returncode\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fail_runs = []\n",
    "\n",
    "# Function to parse Surefire reports and check for flaky tests\n",
    "def check_flaky_tests(project, test_class, report_dir, runs = 10):\n",
    "\n",
    "    for root, _, files in os.walk(report_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".xml\") and test_class in file:\n",
    "                file_path = os.path.join(root, file)\n",
    "                tree = ET.parse(file_path)\n",
    "                xml_root = tree.getroot()\n",
    "\n",
    "                for testcase in xml_root.findall('testcase'):\n",
    "                    name = testcase.get('name')\n",
    "                    name = project + '#' + test_class + '#' + name\n",
    "                    if testcase.find('failure') is not None or testcase.find('error') is not None:\n",
    "                        randoop_test_pattern = r'^test\\d+(_\\d+)$'\n",
    "                        if 'RandoopTest' in test_class or 'FlakyTest' in test_class:\n",
    "                            verified_flaky_tests[project][name] = 0 \n",
    "                            # print(testcase.get('name'))\n",
    "                        else:\n",
    "                            verified_flaky_tests[project][name] += 1\n",
    "                            fail_runs.append(file)\n",
    "    print(fail_runs)\n",
    "    return {test: count for test, count in verified_flaky_tests[project].items() if count > 1 and count < runs}\n",
    "\n",
    "# Main function to run the test class multiple times and check for flakiness\n",
    "def main_check(test_class, project, runs=10 ):\n",
    "\n",
    "    project_dir = PROJECTS_DIRECTORY + project\n",
    "    base_report_dir= project_dir + \"/target/surefire-reports/\"\n",
    "\n",
    "    for i in range(runs):\n",
    "        run_dir = f\"run_{i+1}\"\n",
    "        # report_dir = os.path.join(base_report_dir, run_dir)\n",
    "        # os.makedirs(report_dir, exist_ok=True)\n",
    "\n",
    "        print(f\"Running test {i+1}/{runs} of project {project} ...\")\n",
    "        # result_code = run_maven_test(project_dir, test_class, run_dir)\n",
    "\n",
    "        if result_code != 0:\n",
    "            print(f\"Test run {i+1} failed with return code {result_code}\")\n",
    "            fail_runs.add(run_dir)\n",
    "    verified_flaky_tests = check_flaky_tests(project, test_class, base_report_dir)\n",
    "\n",
    "    if verified_flaky_tests:\n",
    "        print(\"Flaky tests found:\")\n",
    "\n",
    "        for test, count in verified_flaky_tests.items():\n",
    "            verified_tests_set.add(test)\n",
    "            print(f\"Test {test} failed {count} times\")\n",
    "    else:\n",
    "        print(\"No flaky tests found.\")\n",
    "\n",
    "\n",
    "\n",
    "def check_randoop_flaky():\n",
    "    test_class = \"flaky.FlakyTest\"\n",
    "    for flaky_project in flaky_projects:\n",
    "        main_check(test_class, project = flaky_project)\n",
    "\n",
    "\n",
    "def check_all_flaky(project_name):\n",
    "    target_dir =  PROJECTS_DIRECTORY + project_name\n",
    "    for dir, subpath, files in os.walk(target_dir+'/target/test-classes'):\n",
    "        for file in files:\n",
    "            if file.endswith(\".class\"):   #change here to '.class' to make it run all classes\n",
    "                class_name = os.path.join(dir, file)\n",
    "                class_name = class_name.replace(target_dir+'/target/test-classes/','').replace('/', '.').replace('\\\\', '.').replace('.class', '')\n",
    "                print(\"start rerun {project}, {class_name}\")\n",
    "                main_check(class_name, project = project_name)\n",
    "\n",
    "def check_all_flaky_by_single_run(project_name):\n",
    "    main_check(\"haha\", project = project_name)\n",
    "    \n",
    "    \n",
    "def find_verified_flaky(project_name):\n",
    "    # global fail_runs\n",
    "    fail_runs.clear()\n",
    "    fail_log_dir = SUMMARY_LOG+'/verified_fail_log/'\n",
    "\n",
    "\n",
    "    target_dir =  PROJECTS_DIRECTORY + project_name\n",
    "    for dir, subpath, files in os.walk(target_dir+'/target/test-classes'):\n",
    "        for file in files:\n",
    "            if file.endswith(\".class\"):   #change here to '.class' to make it run all classes\n",
    "                class_name = os.path.join(dir, file)\n",
    "                class_name = class_name.replace(target_dir+'/target/test-classes/','').replace('/', '.').replace('\\\\', '.').replace('.class', '')\n",
    "                base_report_dir= target_dir + \"/target/surefire-reports/\"\n",
    "                verified_flaky_tests = check_flaky_tests(project_name, class_name, base_report_dir)\n",
    "                if verified_flaky_tests:\n",
    "                    if not os.path.exists(fail_log_dir+'/'+project_name):\n",
    "                        os.mkdir(fail_log_dir +'/'+project_name)\n",
    "                    print(fail_runs)\n",
    "                    for run in fail_runs:\n",
    "                        print(base_report_dir+\"/\" + run.replace('.xml','.txt'))\n",
    "                        shutil.copy(base_report_dir+\"/\" + run.replace('.xml','.txt').replace('TEST-','') , fail_log_dir +project_name+'/')\n",
    "                    for test, count in verified_flaky_tests.items():\n",
    "                        verified_tests_set.add(test)\n",
    "\n",
    "    \n",
    "analyzed_projects = set()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    os.environ['JAVA_HOME'] = JAVA_HOME\n",
    "    os.environ['MAVEN_HOME'] = MVN_LOC\n",
    "    projects_info, filtered_df= read_dataset()\n",
    "    # os.environ['PATH'] += JAVA_HOME+'/bin/'\n",
    "    # # print(os.environ['PATH'])\n",
    "\n",
    "    # # os.system('mvn -v')\n",
    "    subprocess.run(MVN_LOC+ \" -v\", executable='/bin/zsh', shell=True,\n",
    "                   stderr=subprocess.STDOUT, timeout=90)\n",
    "    # test_project = 'yankeguo-xlog-java'\n",
    "    # if not os.path.exists(PROJECTS_DIRECTORY):\n",
    "    #     os.mkdir(PROJECTS_DIRECTORY)\n",
    "    temp_test_project_list = {'kestreldigital-data-conjuror','DiUS-java-faker','lemire-externalsortinginjava'}\n",
    "    temp_test_project_list = {'kestreldigital-data-conjuror'}\n",
    " \n",
    "    for index, project in projects_info.iterrows():\n",
    "        project_name = project['Project_Name']\n",
    "\n",
    "        target_dir = PROJECTS_DIRECTORY + project_name\n",
    "    #     if not os.path.exists(target_dir):\n",
    "    #         download_project(project, target_dir)\n",
    "    #     if not os.path.exists(target_dir + '/build.log'):\n",
    "        # build_project(target_dir)\n",
    "    #     else:\n",
    "    #         shutil.copy(target_dir + '/build.log',\n",
    "    #                     RANDOOP_GENERATED_DIRECTORY + project['Project_Name'] + '/' + project[\n",
    "    #                         'Project_Hash'] + '/alltests/build.log')\n",
    "\n",
    "    #     # print(os.listdir(target_dir))\n",
    "        # if \"edwardcapriolo-teknek-core\" == project_name or \"mbknor-dropwizard-activemq-bundle\" == project_name:\n",
    "        #     continue\n",
    "      \n",
    "        # run_randoop(project,target_dir)\n",
    "\n",
    "\n",
    "    #     # search_error_cause()\n",
    "    #     # break\n",
    "    #     # os.chdir(PROJECTS_DIRECTORY + project_name)\n",
    "    #     # print(os.getcwd())\n",
    "    # search_error_cause()\n",
    "    find_flaky()\n",
    "    # print('sadasdkasds')\n",
    "    # print(flaky_projects)\n",
    "    # uncomment_tests()\n",
    "    # generate_new_testclass()\n",
    "    # copy_and_run()\n",
    "    # collect_flaky()\n",
    "    # statistic_flaky()\n",
    "\n",
    "    # make_flaky_table()\n",
    "    # target_project = 'BriteSnow-jomni'\n",
    "    # for project_name in temp_test_project_list:\n",
    "    fail_log_dir = SUMMARY_LOG+'/verified_fail_log/'\n",
    "    # if not os.path.exists(fail_log_dir):\n",
    "    #     os.mkdir(fail_log_dir)\n",
    "    \n",
    "    \n",
    "    # turcate = False\n",
    "    # for index, project in projects_info.iterrows():\n",
    "    #     project_name = project['Project_Name']\n",
    "    #     # if os.path.exists(RANDOOP_GENERATED_DIRECTORY + project['Project_Name'] + '/' + project['Project_Hash']+'/alltests/FlakyTest.java'):\n",
    "    #     if project_name != 'StefaniniInspiring-pugtsdb':\n",
    "    #         print(project_name)\n",
    "    #         if project_name == 'mitoma-ponto':\n",
    "    #             turcate = True\n",
    "    #             break\n",
    "    #             #  if not os.path.exists(RANDOOP_GENERATED_DIRECTORY + project['Project_Name'] + '/' + project['Project_Hash']+'/alltests/flakyTracker'):\n",
    "    #         # if project_name == test_project:\n",
    "    # #     #     #         print(project_name)\n",
    "    #         if not turcate:\n",
    "    #             for dir, subpath, files in os.walk(RANDOOP_GENERATED_DIRECTORY + project['Project_Name'] + '/' + project['Project_Hash']+'/alltests/flakyTracker'):\n",
    "                \n",
    "    #                 for file in files:\n",
    "    #                       #see if flakytracker can run something\n",
    "    #                     if file.endswith('.trackerlog'):\n",
    "    #                         analyzed_projects.add(project_name)\n",
    "    #                         break\n",
    "    #         #     run_flaky_tracker(project, PROJECTS_DIRECTORY + project_name)\n",
    "    #             # check_all_flaky_by_single_run(project_name)\n",
    "    #             if project_name in analyzed_projects:\n",
    "    #                 find_verified_flaky(project_name)\n",
    "    parseTrackerLog(RANDOOP_GENERATED_DIRECTORY)\n",
    "    # check_randoop_flaky()\n",
    "    # print(verified_flaky_tests)\n",
    "    # print(analyzed_projects)\n",
    "    for idx, test in filtered_df.iterrows():\n",
    "        if test['Project_Name'] in analyzed_projects:\n",
    "            flaky_name = test['Project_Name'] + '#' + test['Test_class'] + '#' + test ['Test_name']\n",
    "            generated_dataset_labeled_flaky_test.add(flaky_name)\n",
    "    # print('randoop len' ,len(randoop_flaky_test))\n",
    "    # print('tracker len' ,len(tracker_randoop_flaky_test))\n",
    "    # print('verified len', len(verified_tests_set))\n",
    "    # print('random num', random_num)\n",
    "    # print('time num', time_num)\n",
    "    # print('true positive:',len(randoop_flaky_test.intersection(tracker_randoop_flaky_test)),'\\n--------------------\\n', sorted(randoop_flaky_test.intersection(tracker_randoop_flaky_test)))\n",
    "    # print('false positive:?', len(tracker_randoop_flaky_test-randoop_flaky_test),'\\n--------------------\\n',sorted(tracker_randoop_flaky_test-randoop_flaky_test))\n",
    "    # print('false negative:',len(randoop_flaky_test-tracker_randoop_flaky_test),'\\n--------------------\\n',sorted(randoop_flaky_test-tracker_randoop_flaky_test))\n",
    "    print(\"ground truth\", len(verified_tests_set),verified_tests_set)\n",
    "    print('true positive:',len(verified_tests_set.intersection(tracker_flaky_tests_except_randoop)),'\\n--------------------\\n', sorted(verified_tests_set.intersection(tracker_flaky_tests_except_randoop)))\n",
    "    print('false positive:?', len(tracker_flaky_tests_except_randoop-verified_tests_set),'\\n--------------------\\n',sorted(tracker_flaky_tests_except_randoop-verified_tests_set))\n",
    "    print('false negative:',len(verified_tests_set-tracker_flaky_tests_except_randoop),'\\n--------------------\\n',sorted(verified_tests_set-tracker_flaky_tests_except_randoop))\n",
    "\n",
    "    print(\"true positive labeled\",len(generated_dataset_labeled_flaky_test.intersection(tracker_flaky_tests_except_randoop)),'\\n--------------------\\n', sorted(generated_dataset_labeled_flaky_test.intersection(tracker_flaky_tests_except_randoop)))\n",
    "    print(\"false positive labeled\", len(tracker_flaky_tests_except_randoop - generated_dataset_labeled_flaky_test ),'\\n--------------------\\n', sorted(tracker_flaky_tests_except_randoop - generated_dataset_labeled_flaky_test ))\n",
    "    print(\"false nagative labeled\",len(generated_dataset_labeled_flaky_test - tracker_flaky_tests_except_randoop),'\\n--------------------\\n', sorted(generated_dataset_labeled_flaky_test - tracker_flaky_tests_except_randoop))\n",
    "    \n",
    "    # print(flaky_projects)\n",
    "\n",
    "    # search_build_error()\n",
    "    # search_error_cause()\n",
    "    # print(generate_fails.intersection(build_fails))\n",
    "    # print(\"total\",total_test_num)\n",
    "    # print(\"flaky\",total_flaky_num)\n",
    "    # print(len(generate_fails), generate_fails)\n",
    "    # print(len(generate_fails - build_fails), \"build but not generate:\", generate_fails - build_fails)\n",
    "    # print(len(build_fails - generate_fails), \"generate but build fails\", build_fails - generate_fails)\n",
    "    # # print(generate_fails.discard(generate_fails.intersection(build_fails)))\n",
    "    # print(len(build_fails), build_fails)\n",
    "    # print(len(build_fails.intersection(generate_fails)))\n",
    "    # print(\"error case #:\", error_case_num)\n",
    "    # print(len(evosuite_flaky_projects), evosuite_flaky_projects)\n",
    "    # print(len(flaky_projects), flaky_projects)\n",
    "    # for project in evosuite_flaky_projects:\n",
    "    #     print(project)\n",
    "    #\n",
    "    #\n",
    "    # print(\"sum\", len(flaky_projects | evosuite_flaky_projects))\n",
    "    # print(\"only evosuite:\", len(evosuite_flaky_projects - flaky_projects), evosuite_flaky_projects - flaky_projects)\n",
    "    # print(\"only randoop:\", len(flaky_projects - evosuite_flaky_projects), flaky_projects - evosuite_flaky_projects)\n",
    "    # print(len(flaky_projects.intersection(evosuite_flaky_projects)),\n",
    "    #       flaky_projects.intersection(evosuite_flaky_projects))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1603479fc05716f2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def read_idoft():\n",
    "    path = './pr-data.csv'\n",
    "    df = pd.read_csv(path)\n",
    "    need_process_projects = df[['Project URL', 'SHA Detected']].loc[\n",
    "        (df['Category'] == 'UD') |\n",
    "        (df['Category'] == 'NOD') |\n",
    "        # (df['Category'] == 'NIO') |\n",
    "        (df['Category'] == 'OSD')\n",
    "        ].drop_duplicates()\n",
    "    need_process_projects = need_process_projects.rename(columns={'SHA Detected': 'Project_Hash','Project URL':'Project_URL'})\n",
    "    need_process_projects['Project_Name'] = need_process_projects['Project_URL'].apply(lambda x : x.split('/')[-1])\n",
    "    print(need_process_projects)\n",
    "    get_tests_to_focus(df)\n",
    "\n",
    "def get_tests_to_focus(df):\n",
    "    need_process_projects = df[['Project URL', 'SHA Detected','Module Path', 'Fully-Qualified Test Name (packageName.ClassName.methodName)']].loc[\n",
    "        (df['Category'] == 'UD') |\n",
    "        (df['Category'] == 'NOD') |\n",
    "        # (df['Category'] == 'NIO') |\n",
    "        (df['Category'] == 'OSD')]\n",
    "    need_process_projects = need_process_projects.rename(columns = {'SHA Detected': 'Project_Hash','Project URL':'Project_URL','Module Path':'Module', 'Fully-Qualified Test Name (packageName.ClassName.methodName)':'testname'})\n",
    "    need_process_projects['Project_Name'] = need_process_projects['Project_URL'].apply(lambda x: x.split('/')[-1])\n",
    "    need_process_projects = need_process_projects.drop(columns = ['Project_URL'])\n",
    "    print(need_process_projects)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce066adbf9ee2fa9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
